{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ptIhcAexGZ",
        "outputId": "a75f6486-9fe5-4326-f922-dcd532b56296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_root = '/content/drive/My Drive/Chatbot'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing relevant libraries\n",
        "\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn-5q8rIg68y",
        "outputId": "aa9073a9-7963-4aa0-be09-978d51662616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Dataset: intens.json\n",
        "\n",
        "data_file = open(data_root + '/intents.json').read()\n",
        "data = json.loads(data_file)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oZD39hPkGFw",
        "outputId": "7fce2699-7a8e-470f-e081-4a0011bd55d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'hello',\n",
              "   'patterns': ['Hello', 'Hi there', 'Good morning', \"What's up\"],\n",
              "   'responses': ['Hey!', 'Hello', 'Hi!', 'Good morning!'],\n",
              "   'context': ''},\n",
              "  {'tag': 'noanswer',\n",
              "   'patterns': [],\n",
              "   'responses': [\"Sorry, can't understand you\",\n",
              "    'Please give me more info',\n",
              "    'Not sure I understand'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'job',\n",
              "   'patterns': ['What is your job', 'What is your work'],\n",
              "   'responses': ['My job is to make you feel like everything is okay.',\n",
              "    'I work to serve you as well as possible'],\n",
              "   'context': ''},\n",
              "  {'tag': 'age',\n",
              "   'patterns': ['What is your age', 'How old are you', 'When were you born'],\n",
              "   'responses': ['I was born in 2021'],\n",
              "   'context': ''},\n",
              "  {'tag': 'feeling',\n",
              "   'patterns': ['How are you today', 'How are you'],\n",
              "   'responses': ['I am feeling good, you?',\n",
              "    'Very good and you?',\n",
              "    \"Actually, I'm okay and you?\"],\n",
              "   'context': ''},\n",
              "  {'tag': 'good',\n",
              "   'patterns': ['I am good too',\n",
              "    'I feel fine',\n",
              "    'Good !',\n",
              "    'Fine',\n",
              "    'I am good',\n",
              "    'I am great',\n",
              "    'great'],\n",
              "   'responses': ['That is perfect!', \"So, everything's okay!\"],\n",
              "   'context': 'feeling'},\n",
              "  {'tag': 'bad',\n",
              "   'patterns': ['I am feeling bad', 'No I am sad', 'No'],\n",
              "   'responses': ['I hope you will feel better !'],\n",
              "   'context': 'feeling'},\n",
              "  {'tag': 'actions',\n",
              "   'patterns': ['What can you do', 'What can I ask you', 'Can you help me'],\n",
              "   'responses': ['I can do a lot of things but here are some of my skills, you can ask me: the capital of a country, its currency and its area. A random number. To calculate a math operation.'],\n",
              "   'context': ''},\n",
              "  {'tag': 'women',\n",
              "   'patterns': ['Are you a girl', 'You are a women'],\n",
              "   'responses': ['Sure, I am a women'],\n",
              "   'context': ''},\n",
              "  {'tag': 'men',\n",
              "   'patterns': ['Are you a men', 'Are you a boy'],\n",
              "   'responses': ['No, I am a women'],\n",
              "   'context': ''},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thank you', 'Thank you very much', 'thanks'],\n",
              "   'responses': ['I only do my jobï¸', 'No problem!'],\n",
              "   'context': ''},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Goodbye', 'Good afternoon', 'Bye'],\n",
              "   'responses': ['Goodbye!', 'See you soon!'],\n",
              "   'context': ''},\n",
              "  {'tag': 'city',\n",
              "   'patterns': ['Where do you live'],\n",
              "   'responses': ['I live in a server located in the US!'],\n",
              "   'context': ''},\n",
              "  {'tag': 'action',\n",
              "   'patterns': ['What are you doing'],\n",
              "   'responses': [\"Actually, I'm chatting with somebody\"],\n",
              "   'context': ''},\n",
              "  {'tag': 'wait',\n",
              "   'patterns': ['Can you wait 2 minutes', 'Please wait', 'Wait 2 secs please'],\n",
              "   'responses': ['Sure! I wait.'],\n",
              "   'context': ''},\n",
              "  {'tag': 'still there',\n",
              "   'patterns': ['Are you still there?', 'Are you here?'],\n",
              "   'responses': ['Of course! Always at your service.'],\n",
              "   'context': ''}]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracing data_x (features) and data y (target)\n",
        "\n",
        "words = [] #For bow model/ vocabulary for patterns\n",
        "classes = [] #For bow model/ vocabulary for tags\n",
        "data_x = [] #For storing each pattern\n",
        "data_y = [] #For storing tag corresponding to each pattern in data_X\n",
        "#Iterating over all the intents\n",
        "\n",
        "for intent in data [\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.word_tokenize(pattern) # tokenize each pattern\n",
        "    words.extend(tokens) #and append tokens to word\n",
        "    data_x.append(pattern) #appending pattern to data_X\n",
        "    data_y.append(intent[\"tag\"]), #appending the associated tag to each pattern\n",
        "\n",
        "    #adding the tag to the classes if it's not there already\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])\n",
        "\n",
        "#initializing lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "\n",
        "word = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjQWY1DZlzQO",
        "outputId": "8c3f8939-174d-416f-bc09-d29c620b4e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['action',\n",
              " 'actions',\n",
              " 'age',\n",
              " 'bad',\n",
              " 'city',\n",
              " 'feeling',\n",
              " 'good',\n",
              " 'goodbye',\n",
              " 'hello',\n",
              " 'job',\n",
              " 'men',\n",
              " 'still there',\n",
              " 'thanks',\n",
              " 'wait',\n",
              " 'women']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text to numbers\n",
        "\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "\n",
        "#creating the bag of words model\n",
        "\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "\n",
        "  #mark the index of class that the current pattern is associated\n",
        "  #to\n",
        "\n",
        "  output_row = list(out_empty)\n",
        "  output_row[classes.index(data_y[idx])]\n",
        "\n",
        "  #add the one hot encoded Bow and associated classes training\n",
        "\n",
        "  training.append ([bow, output_row])\n",
        "\n",
        "#shuffle the data and convert it to an array\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "#split the features and target labels\n",
        "\n",
        "train_x = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:,1]))\n",
        "\n",
        "train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao2pWpGYse1P",
        "outputId": "c813ce4d-7098-4997-87e7-7bf149886daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential ()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation = \"softmax\"))\n",
        "adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6)\n",
        "model.compile(loss = \"categorical_crossentropy\",\n",
        "              optimizer=adam,\n",
        "              metrics =['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(x=train_x, y = train_y, epochs=1000, verbose=1)"
      ],
      "metadata": {
        "id": "a7duOnyRw-a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0] #Extracting probabilities\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True) #Sorting by values of probability in decreasing order\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Sorry! I don't understand.\"\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result"
      ],
      "metadata": {
        "id": "IP8WbOj32ibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interacting with the chatbot\n",
        "print(\"Press 0 if you don't want to chat with our ChatBot.\")\n",
        "while True:\n",
        "    message = input(\"\")\n",
        "    if message == \"0\":\n",
        "      break\n",
        "    intents = pred_class(message, words, classes)\n",
        "    result = get_response(intents, data)\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6nijamA27yN",
        "outputId": "9df48819-63ba-4d79-f24b-36b2c24f24f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press 0 if you don't want to chat with our ChatBot.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "My job is to make you feel like everything is okay.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "My job is to make you feel like everything is okay.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "My job is to make you feel like everything is okay.\n"
          ]
        }
      ]
    }
  ]
}